# LocalAI model configuration for GLM-4.7
# Backend: llama-cpp with CUDA support
# Note: GLM-4 requires using the model's built-in Jinja template

- name: GLM-4.7-Q8_0
  backend: cuda13-llama-cpp
  options:
      - use_jinja:true
  parameters:
    model: GLM-4.7-Q8_0/GLM-4.7-Q8_0-00001-of-00008.gguf
    f16_kv: true
    temperature: 0.7
    top_p: 0.9
    top_k: 40
    max_tokens: 2048
  threads: 8
  context_size: 16384
  gpu_layers: 99
  template:
    use_tokenizer_template: true
    # Enable Jinja template support (required for GLM-4)

- name: GLM-4.7-Flash-Q8_0
  backend: llama-cpp
  description: Imported from huggingface://unsloth/GLM-4.7-Flash-GGUF/GLM-4.7-Flash-Q8_0.gguf
  function:
      grammar:
          disable: true
  known_usecases:
      - chat
  options:
      - use_jinja:true
  parameters:
      model: GLM-4.7-Flash-Q8_0/GLM-4.7-Flash-Q8_0.gguf
      temperature: 1.0
      top_p: 1.0
      min_p: 0.01
  template:
      use_tokenizer_template: true
