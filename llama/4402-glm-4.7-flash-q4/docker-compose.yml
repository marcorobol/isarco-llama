version: '3.8'

services:
  glm-4-7-flash:
    image: docker://ghcr.io/ggerganov/llama.cpp:latest-cuda
    container_name: glm-4-7-flash-server
    hostname: glm-4-7-flash-server

    # GPU configuration
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

    # Volumes
    volumes:
      - /data/models:/models:ro

    # Ports
    ports:
      - "4402:4402"

    # Environment
    environment:
      - CUDA_VISIBLE_DEVICES=0,1,2,3

    # Command
    # GLM-4.7-Flash (30B MoE, ~3.6B active parameters)
    # Context window: up to 202,752 tokens
    # Required flags:
    #   --jinja          - Required for correct chat template handling
    #   --fit on         - Optimize GPU utilization across all available VRAM
    #   --repeat-penalty 1.0 - Disable repeat penalty (recommended for GLM models)
    #   --min-p 0.01     - Additional parameter for GLM-4.7-Flash
    command: >
      /opt/llama.cpp/build/bin/llama-server
      -m /models/GLM-4.7-Flash-GGUF/GLM-4.7-Flash-UD-Q4_K_XL.gguf
      -ngl 99
      -c 65536
      --port 4402
      --host 0.0.0.0
      -t 32
      --jinja
      --fit on
      --temp 1.0
      --top-p 0.95
      --min-p 0.01
      --repeat-penalty 1.0

    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4402/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

    # Restart policy
    restart: unless-stopped
