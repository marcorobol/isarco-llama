version: '3.8'

services:
  glm-4-7:
    image: docker://ghcr.io/ggerganov/llama.cpp:latest-cuda
    container_name: glm-4-7-server
    hostname: glm-4-7-server

    # GPU configuration
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

    # Volumes
    volumes:
      - /data/models:/models:ro

    # Ports
    ports:
      - "4401:4401"

    # Environment
    environment:
      - CUDA_VISIBLE_DEVICES=0,1,2,3

    # Command
    # GLM-4.7 (355B parameter MoE model)
    # Context window: up to 131,072 tokens
    # Required flags:
    #   --jinja          - Required for correct chat template handling
    #   --fit on         - Optimize GPU utilization across all available VRAM
    #   --repeat-penalty 1.0 - Disable repeat penalty (recommended for GLM models)
    command: >
      /opt/llama.cpp/build/bin/llama-server
      -m /models/GLM-4.7-GGUF/UD-Q2_K_XL/GLM-4.7-UD-Q2_K_XL-00001-of-00003.gguf
      -ngl 99
      -c 65536
      --port 4401
      --host 0.0.0.0
      -t 32
      --jinja
      --fit on
      --temp 1.0
      --top-p 0.95
      --repeat-penalty 1.0

    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4401/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

    # Restart policy
    restart: unless-stopped
